{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Leaf_dataset.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrAAO1CuwDxozQ1EXapOW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Porubova/Leaf-dataset/blob/master/Leaf_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOZJvUuD_jif",
        "colab_type": "text"
      },
      "source": [
        "Leafsnap (http://leafsnap.com) is an electronic field guide for identifying tree\n",
        "species from photos of their leaves. It was jointly created by computer\n",
        "scientists from Columbia University and the University of Maryland, and\n",
        "botanists from the Smithsonian Institution in Washington, DC.\n",
        "The dataset covers all 185 tree species from the Northeastern United States.\n",
        "\n",
        "Ref: \n",
        "\n",
        "Neeraj Kumar, Peter N. Belhumeur, Arijit Biswas, David W. Jacobs, W. John Kress, Ida Lopez, JoÃ£o V. B. Soares,\n",
        "\"Leafsnap: A Computer Vision System for Automatic Plant Species Identification,\"\n",
        "Proceedings of the 12th European Conference on Computer Vision (ECCV),\n",
        "October 2012."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVnqQep2tfzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#donload libraries\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms, models, utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCAR5Lrsrclw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#donload dataset from source\n",
        "!wget http://leafsnap.com/static/dataset/leafsnap-dataset.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHabPhRBr7Hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract tar archieve \n",
        "!tar -xf leafsnap-dataset.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQBP_Hd1u1Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create data folder for merged lab and field images\n",
        "if not os.path.exists(\"data\"):\n",
        "  os.mkdir('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06aAv4i8ANm4",
        "colab_type": "text"
      },
      "source": [
        "The images stored in into two categories:\n",
        "\n",
        "1. 23147 Lab images, consisting of high-quality images taken of pressed leaves, from the Smithsonian collection. These images appear in controlled backlit and front-lit versions, with several samples per species.\n",
        "\n",
        "2. 7719 Field images, consisting of \"typical\" images taken by mobile devices 2.2(iPhones mostly) in outdoor environments. These images contain varying amounts of blur, noise, illumination patterns, shadows, etc.\n",
        "\n",
        "I merged them into one folder, separated by names.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfwTBhCgwvPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Easy and elegant merge of two folders with subfolders and files https://lukelogbook.tech/2018/01/25/merging-two-folders-in-python/\n",
        "\n",
        "def mergefolders(root_src_dir, root_dst_dir):\n",
        "    for src_dir, dirs, files in os.walk(root_src_dir):\n",
        "        dst_dir = src_dir.replace(root_src_dir, root_dst_dir, 1)\n",
        "        if not os.path.exists(dst_dir):\n",
        "            os.makedirs(dst_dir)\n",
        "        for file_ in files:\n",
        "            src_file = os.path.join(src_dir, file_)\n",
        "            dst_file = os.path.join(dst_dir, file_)\n",
        "            if os.path.exists(dst_file):\n",
        "                os.remove(dst_file)\n",
        "            shutil.copy(src_file, dst_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypbB5ZAIw0v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mergefolders('/content/dataset/images/field','data')\n",
        "mergefolders('/content/dataset/images/lab','data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf5EHNt_yP41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.make_archive('leaf_data', 'zip', 'data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VebgEpBZBMXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = os.listdir('/content/data')\n",
        "classes.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4xRdBoHCAFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of classes per leaf dataset:\",len(classes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMESFcjn1Kn1",
        "colab_type": "text"
      },
      "source": [
        "let's print number of images for the first 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeWQJ5oXCTwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for cla in classes[0:10]:\n",
        "  print (cla, \": number of images \", len(os.listdir('data/'+cla)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4jJ9tuz1XQY",
        "colab_type": "text"
      },
      "source": [
        "Next, we can display random lab and field images to see how they differ in size and quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0knVqHXpOoFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper display function\n",
        "def imshow_raw(img, title = None):\n",
        "    img = mpimg.imread(img)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.imshow(img)\n",
        "    \n",
        "#random images from dataset\n",
        "random_images =['/content/data/abies_nordmanniana/13291651129246.jpg', \n",
        "         '/content/data/abies_nordmanniana/ny1057-01-3.jpg', \n",
        "         '/content/data/betula_lenta/1248106459_0000.jpg',\n",
        "         '/content/data/betula_lenta/wb1193-06-4.jpg',\n",
        "         '/content/data/juglans_nigra/1249316300_0002.jpg',\n",
        "         '/content/data/juglans_nigra/pi2116-03-3.jpg',\n",
        "         '/content/data/nyssa_sylvatica/12992004532153.jpg',\n",
        "         '/content/data/nyssa_sylvatica/ny1128-02-4.jpg',\n",
        "         '/content/data/quercus_montana/13292231174757.jpg',\n",
        "         '/content/data/quercus_montana/pi0037-03-4.jpg',\n",
        "         '/content/data/quercus_montana/pi0037-03-2.jpg']        \n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(20,4))\n",
        "plot_size=10\n",
        "for i in np.arange(plot_size):\n",
        "    ax = fig.add_subplot(2, plot_size/2, i+1, xticks=[], yticks=[])\n",
        "    imshow_raw(random_images[i], random_images[i].split('/')[-2].title())\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5dP7nW62MxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'data'\n",
        "# arrays to normalization\n",
        "normalize_mean = np.array([0.485, 0.456, 0.406])\n",
        "normalize_std = np.array([0.229, 0.224, 0.225])\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(normalize_mean,\n",
        "        normalize_std)])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        normalize_mean,\n",
        "        normalize_std)])\n",
        "    \n",
        "train_dataset = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(data_dir, transform=test_transforms)\n",
        "#dataset.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDzyR0atqrCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initiate parameters for splitting our data\n",
        "num_workers = 1\n",
        "batch_size =32\n",
        "valid_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "num_train = len(dataset)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "split = int(np.floor(0.8 * num_train))\n",
        "split_1 = int(np.floor(0.1 * num_train))\n",
        "split_2 = split+split_1\n",
        "\n",
        "train_idx, valid_idx, test_idx = (indices[:split], indices[split:split_2],\n",
        "                                  indices[split_2+1:])\n",
        "\n",
        "# define samplers for obtaining training, validation and test batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler =  SubsetRandomSampler(test_idx)\n",
        "# load our batches\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           sampler=train_sampler, \n",
        "                                           num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           sampler=valid_sampler,\n",
        "                                           num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           sampler=test_sampler,                                       \n",
        "                                           num_workers=num_workers)\n",
        "len(train_idx), len(indices),len(valid_idx), len(test_idx)\n",
        "print(\"Length of dataset: \",len(indices))\n",
        "print(\"Length of training set: \",len(train_idx))\n",
        "print(\"Length of validation set: \",len(valid_idx))\n",
        "print(\"Length of test set: \",len(test_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkPDKNURF5Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img, title=None):\n",
        "    # un-normalize\n",
        "    for i in range(img.shape[0]):\n",
        "        img[i] = img[i] * normalize_std[i] + normalize_mean[i]\n",
        "    \n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SCiTsX_Tsq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#display training batch\n",
        "dataiter = iter(train_loader)\n",
        "\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "labels = labels.tolist()\n",
        "\n",
        "print(\"Batch shape:\",images.shape)\n",
        "fig = plt.figure(figsize=(20,11))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(4, batch_size/4, idx+1, xticks=[], yticks=[])    \n",
        "    imshow(images[idx], classes[labels[idx]].title())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saX8QLTeMy99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA available.  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Ss-faXGjCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #shorter version of VGG 16 CNN architecture\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "        \n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)   \n",
        "#         self.conv2 = nn.Conv2d(6, 12, 3, padding=1)      \n",
        "#         self.conv3 = nn.Conv2d(12, 24, 3, padding=1)\n",
        "#         self.conv4 = nn.Conv2d(24, 48, 3, padding=1)      \n",
        "#         self.conv5 = nn.Conv2d(48, 96, 3, padding=1)\n",
        "#         self.pool = nn.MaxPool2d(2,2)       \n",
        "#         self.fc1 = nn.Linear(13*13*96, 4096)       \n",
        "#         self.fc2 = nn.Linear(4096 , 4096)\n",
        "#         self.fc3 = nn.Linear(4096, 185)       \n",
        "#         self.dropout = nn.Dropout(0.01)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # add sequence of convolutional and max pooling layers\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = self.pool(F.relu(self.conv5(x)))\n",
        "#         # flatten image input and add hidden layers with dropout and\n",
        "#         # activation function ReLu\n",
        "#         #print(\"conv output shape\", x.shape)\n",
        "#         x = self.dropout(x.view(-1,  13*13*96))     \n",
        "#         x = self.dropout(F.relu(self.fc1(x)))       \n",
        "#         x = self.dropout(F.relu(self.fc2(x)))        \n",
        "#         x = self.fc3(x)\n",
        "        \n",
        "#         return x\n",
        "\n",
        "# model = Net()\n",
        "# # move tensors to GPU if CUDA is available\n",
        "# if train_on_gpu:\n",
        "#     model.cuda()\n",
        "# print(model)\n",
        "# #loss function for custom cnn\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# #optimizer\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.03)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz_orjDEGmn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #loss function for custom cnn\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# #optimizer\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.03)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z82NMYDuwCZp",
        "colab_type": "text"
      },
      "source": [
        "### VGG-19 pre-trained **model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVMjUZsBdosM",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pZZ9oXVqRxk",
        "colab_type": "text"
      },
      "source": [
        "Load and freeze parameters in VGG19 pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJGIT3_Tjq-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.vgg19(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVeqXf6rjthy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "from collections import OrderedDict\n",
        "from collections import OrderedDict\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(25088, 500)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('dr', nn.Dropout(p=0.1)),\n",
        "                          ('fc2', nn.Linear(500, 185)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "                          \n",
        "    \n",
        "model.classifier = classifier\n",
        "criterion =  nn.NLLLoss()\n",
        "\n",
        "# Only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = optim.SGD(model.classifier.parameters(), lr=0.08)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device);\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OccjkoOl2i1L",
        "colab_type": "text"
      },
      "source": [
        "It took me around 2 hours to train the model on GPU with a batch size of 32. Also, I haven't tried any other optimizers and learning rate was constant 0.05 through training. So, there is room for improvement. Just to mention, the model trained better without extra transformation(flip, rotation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ83TkCjG3Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 20\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0    \n",
        "    \n",
        "    # train the model    \n",
        "    \n",
        "    model.train()\n",
        "    for data, target in train_loader:        \n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()       \n",
        "        optimizer.zero_grad()        \n",
        "        output = model(data)  \n",
        "        #print(output.shape)     \n",
        "        loss = criterion(output, target)       \n",
        "        loss.backward()       \n",
        "        optimizer.step()       \n",
        "        train_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # validate the model \n",
        "  \n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "       \n",
        "        output = model(data)       \n",
        "        loss = criterion(output, target)       \n",
        "        valid_loss += loss.item()*data.size(0)    \n",
        "   \n",
        "    train_loss = train_loss/len(train_idx)\n",
        "    valid_loss = valid_loss/len(valid_idx)        \n",
        "   \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_sl.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1mqBqaDqDSK",
        "colab_type": "text"
      },
      "source": [
        "###Test the model\n",
        "\n",
        "Test and visualize our model. Credits to Udasity for multi classes validation and plotting. GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJQNc_vNp4Kt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(185))\n",
        "class_total = list(0. for i in range(185))\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "   \n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(15):\n",
        "        label = target[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "# # average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(batch_size):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_QeaNz9szgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if train_on_gpu:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(20, 11))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(4, batch_size/4, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images.cpu()[idx])\n",
        "    ax.set_title(\"{} \\n({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34a-w8rhQ1gH",
        "colab_type": "text"
      },
      "source": [
        "download silver maple (Acer platanoides) leaf image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEuH_3-LOfNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_img = !wget http://www.tree-guide.com/images/styles/600x450-copy_/public/silver-maple-leaf.jpg\n",
        "\n",
        "image = Image.open('silver-maple-leaf.jpg')\n",
        "image = process_image(image)\n",
        "imshow(image)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfzDZM0wRbcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(image_path, model, topk=5):\n",
        "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
        "    '''\n",
        "    \n",
        "    # Implement the code to predict the class from an image file\n",
        "    image = Image.open(image_path)\n",
        "    image = process_image(image)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        \n",
        "        image = image.view(1,3,224,224)\n",
        "        image = image.to(device)\n",
        "        \n",
        "        predictions = model.forward(image)\n",
        "        \n",
        "        predictions = torch.exp(predictions)\n",
        "        top_ps, top_class = predictions.topk(topk, dim=1)\n",
        "    \n",
        "    return top_ps, top_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSQ9CbDLRKFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_image(image):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "    \n",
        "    # Process a PIL image for use in a PyTorch model\n",
        "    image = TF.resize(image, 256)\n",
        "    print(image)\n",
        "    upper_pixel = (image.height - 224) // 2\n",
        "    left_pixel = (image.width - 224) // 2\n",
        "    image = TF.crop(image, upper_pixel, left_pixel, 224, 224)\n",
        "    \n",
        "    image = TF.to_tensor(image)\n",
        "    image = TF.normalize(image, normalize_mean, normalize_std)\n",
        "   \n",
        "    \n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ghHtECgQ-Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probs, classez = predict('silver-maple-leaf.jpg', model)\n",
        "print(probs)\n",
        "print(classez)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ox7jh-DUEm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Display an image along with the top 5 classes\n",
        "probs, classez = predict('silver-maple-leaf.jpg', model)\n",
        "\n",
        "probs = probs.data.cpu()\n",
        "probs = probs.numpy().squeeze()\n",
        "\n",
        "classez = classez.data.cpu()\n",
        "classez = classez.numpy().squeeze().tolist()\n",
        "\n",
        "\n",
        "label = classez[0]\n",
        "labels=[classes[l].title() for l in classez]\n",
        "title = classes[label].title()\n",
        "\n",
        "fig = plt.figure(figsize=(4, 10))\n",
        "\n",
        "ax1 = fig.add_subplot(2, 1, 1, xticks=[], yticks=[])\n",
        "\n",
        "image = Image.open('silver-maple-leaf.jpg')\n",
        "image = process_image(image)\n",
        "print(\"This is an image of\",title)\n",
        "imshow(image, \"Acer Platanoides\")\n",
        "\n",
        "ax2 = fig.add_subplot(2, 1, 2, xticks=[], yticks=[])\n",
        "ax2.barh(np.arange(5), probs)\n",
        "ax2.set_yticks(np.arange(5))\n",
        "ax2.set_yticklabels(labels)\n",
        "ax2.set_ylim(-1, 5)\n",
        "ax2.invert_yaxis()\n",
        "ax2.set_xlim(0, 1.1)\n",
        "ax2.set_title('Top 5 predicted')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvYS5fPlQ0Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}