{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Leaf_dataset.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0hWY3tFXZf9a2873HhGRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Porubova/Leaf-dataset/blob/master/Leaf_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOZJvUuD_jif",
        "colab_type": "text"
      },
      "source": [
        "Leafsnap (http://leafsnap.com) is an electronic field guide for identifying tree\n",
        "species from photos of their leaves. It was jointly created by computer\n",
        "scientists from Columbia University and the University of Maryland, and\n",
        "botanists from the Smithsonian Institution in Washington, DC.\n",
        "The dataset covers all 185 tree species from the Northeastern United States.\n",
        "\n",
        "Ref: \n",
        "\n",
        "Neeraj Kumar, Peter N. Belhumeur, Arijit Biswas, David W. Jacobs, W. John Kress, Ida Lopez, JoÃ£o V. B. Soares,\n",
        "\"Leafsnap: A Computer Vision System for Automatic Plant Species Identification,\"\n",
        "Proceedings of the 12th European Conference on Computer Vision (ECCV),\n",
        "October 2012."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVnqQep2tfzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#donload libraries\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms, models, utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCAR5Lrsrclw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#donload dataset from source\n",
        "!wget http://leafsnap.com/static/dataset/leafsnap-dataset.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHabPhRBr7Hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract tar archieve \n",
        "!tar -xf leafsnap-dataset.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQBP_Hd1u1Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create data folder for merged lab and field images\n",
        "if not os.path.exists(\"data\"):\n",
        "  os.mkdir('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06aAv4i8ANm4",
        "colab_type": "text"
      },
      "source": [
        "The images stored in into two categories:\n",
        "\n",
        "1. 23147 Lab images, consisting of high-quality images taken of pressed leaves, from the Smithsonian collection. These images appear in controlled backlit and front-lit versions, with several samples per species.\n",
        "\n",
        "2. 7719 Field images, consisting of \"typical\" images taken by mobile devices 2.2(iPhones mostly) in outdoor environments. These images contain varying amounts of blur, noise, illumination patterns, shadows, etc.\n",
        "\n",
        "I merged them into one folder, separated by names.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfwTBhCgwvPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Easy and elegant merge of two folders with subfolders and files https://lukelogbook.tech/2018/01/25/merging-two-folders-in-python/\n",
        "\n",
        "def mergefolders(root_src_dir, root_dst_dir):\n",
        "    for src_dir, dirs, files in os.walk(root_src_dir):\n",
        "        dst_dir = src_dir.replace(root_src_dir, root_dst_dir, 1)\n",
        "        if not os.path.exists(dst_dir):\n",
        "            os.makedirs(dst_dir)\n",
        "        for file_ in files:\n",
        "            src_file = os.path.join(src_dir, file_)\n",
        "            dst_file = os.path.join(dst_dir, file_)\n",
        "            if os.path.exists(dst_file):\n",
        "                os.remove(dst_file)\n",
        "            shutil.copy(src_file, dst_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypbB5ZAIw0v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mergefolders('/content/dataset/images/field','data')\n",
        "mergefolders('/content/dataset/images/lab','data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf5EHNt_yP41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.make_archive('leaf_data', 'zip', 'data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VebgEpBZBMXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = os.listdir('/content/data')\n",
        "classes.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4xRdBoHCAFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of classes per leaf dataset:\",len(classes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMESFcjn1Kn1",
        "colab_type": "text"
      },
      "source": [
        "let's print number of images for the first 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeWQJ5oXCTwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for cla in classes[0:10]:\n",
        "  print (cla, \": number of images \", len(os.listdir('data/'+cla)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4jJ9tuz1XQY",
        "colab_type": "text"
      },
      "source": [
        "Next, we can display random lab and field images to see how they differ in size and quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0knVqHXpOoFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper display function\n",
        "def imshow(img):\n",
        "    img = mpimg.imread(img)\n",
        "    plt.imshow(img)\n",
        "#random images from dataset\n",
        "random_images =['/content/data/abies_nordmanniana/13291651129246.jpg', \n",
        "         '/content/data/abies_nordmanniana/ny1057-01-3.jpg', \n",
        "         '/content/data/betula_lenta/1248106459_0000.jpg',\n",
        "         '/content/data/betula_lenta/wb1193-06-4.jpg',\n",
        "         '/content/data/juglans_nigra/1249316300_0002.jpg',\n",
        "         '/content/data/juglans_nigra/pi2116-03-3.jpg',\n",
        "         '/content/data/nyssa_sylvatica/12992004532153.jpg',\n",
        "         '/content/data/nyssa_sylvatica/ny1128-02-4.jpg',\n",
        "         '/content/data/quercus_montana/13292231174757.jpg',\n",
        "         '/content/data/quercus_montana/pi0037-03-4.jpg',\n",
        "         '/content/data/quercus_montana/pi0037-03-2.jpg']        \n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(20,4))\n",
        "plot_size=10\n",
        "for i in np.arange(plot_size):\n",
        "    ax = fig.add_subplot(2, plot_size/2, i+1, xticks=[], yticks=[])\n",
        "    imshow(random_images[i])\n",
        "    ax.set_title(random_images[i].split('/')[-2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5dP7nW62MxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'data'\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "#dataset.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDzyR0atqrCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initiate parameters for splitting our data\n",
        "num_workers = 1\n",
        "batch_size =512\n",
        "valid_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "num_train = len(dataset)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "split = int(np.floor(0.8 * num_train))\n",
        "split_1 = int(np.floor(0.1 * num_train))\n",
        "split_2 = split+split_1\n",
        "\n",
        "train_idx, valid_idx, test_idx = (indices[:split], indices[split:split_2],\n",
        "                                  indices[split_2+1:])\n",
        "\n",
        "# define samplers for obtaining training, validation and test batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler =  SubsetRandomSampler(test_idx)\n",
        "# load our batches\n",
        "train_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           sampler=train_sampler, \n",
        "                                           num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           sampler=valid_sampler,\n",
        "                                           num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           sampler=test_sampler,                                       \n",
        "                                           num_workers=num_workers)\n",
        "len(train_idx), len(indices),len(valid_idx), len(test_idx)\n",
        "print(\"Length of dataset: \",len(indices))\n",
        "print(\"Length of training set: \",len(train_idx))\n",
        "print(\"Length of validation set: \",len(valid_idx))\n",
        "print(\"Length of test set: \",len(test_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkPDKNURF5Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SCiTsX_Tsq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "labels = labels.tolist()\n",
        "\n",
        "print(\"Batch shape:\",images.shape)\n",
        "fig = plt.figure(figsize=(20,5))\n",
        "for idx in np.arange(16):\n",
        "    ax = fig.add_subplot(2, 16/2, idx+1, xticks=[], yticks=[])\n",
        "    \n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saX8QLTeMy99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA available.  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Ss-faXGjCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)   \n",
        "        self.conv2 = nn.Conv2d(6, 12, 3, padding=1)      \n",
        "        self.conv3 = nn.Conv2d(12, 24, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(24, 48, 3, padding=1)      \n",
        "        self.conv5 = nn.Conv2d(48, 96, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)       \n",
        "        self.fc1 = nn.Linear(13*13*96, 4096)       \n",
        "        self.fc2 = nn.Linear(4096 , 4096)\n",
        "        self.fc3 = nn.Linear(4096, 185)       \n",
        "        self.dropout = nn.Dropout(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "        # flatten image input and add hidden layers with dropout and\n",
        "        # activation function ReLu\n",
        "        #print(\"conv output shape\", x.shape)\n",
        "        x = self.dropout(x.view(-1,  13*13*96))     \n",
        "        x = self.dropout(F.relu(self.fc1(x)))       \n",
        "        x = self.dropout(F.relu(self.fc2(x)))        \n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJGIT3_Tjq-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Build and train your network\n",
        "model = models.vgg19(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVeqXf6rjthy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "from collections import OrderedDict\n",
        "from collections import OrderedDict\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(25088, 500)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('dr', nn.Dropout(p=0.1)),\n",
        "                          ('fc2', nn.Linear(500, 185)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "                          \n",
        "    \n",
        "model.classifier = classifier\n",
        "criterion =  nn.NLLLoss()\n",
        "\n",
        "# Only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = optim.SGD(model.classifier.parameters(), lr=0.05)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVMjUZsBdosM",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1439/1*_vGloND6yyxFeFH5UyCDVg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz_orjDEGmn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ83TkCjG3Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 20\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0    \n",
        "    \n",
        "    # train the model    \n",
        "    \n",
        "    model.train()\n",
        "    for data, target in train_loader:        \n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()       \n",
        "        optimizer.zero_grad()        \n",
        "        output = model(data)  \n",
        "        #print(output.shape)     \n",
        "        loss = criterion(output, target)       \n",
        "        loss.backward()       \n",
        "        optimizer.step()       \n",
        "        train_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # validate the model \n",
        "  \n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "       \n",
        "        output = model(data)       \n",
        "        loss = criterion(output, target)       \n",
        "        valid_loss += loss.item()*data.size(0)    \n",
        "   \n",
        "    train_loss = train_loss/len(train_idx)\n",
        "    valid_loss = valid_loss/len(valid_idx)        \n",
        "   \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_sl.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC-4rL1IEjln",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}